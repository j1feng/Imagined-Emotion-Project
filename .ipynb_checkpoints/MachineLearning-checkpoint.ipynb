{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 66) (1477, 66) (310, 66) (310, 66) (253, 66) (253, 66) (253, 66)\n",
      "(3282, 66)\n",
      "Number of Statuses: 27\n"
     ]
    }
   ],
   "source": [
    "rec1 = pd.read_csv('rec1.csv')\n",
    "rec2 = pd.read_csv('rec2.csv')\n",
    "rec3 = pd.read_csv('rec3.csv')\n",
    "rec4 = pd.read_csv('rec4.csv')\n",
    "rec5 = pd.read_csv('rec5.csv')\n",
    "rec8 = pd.read_csv('rec8.csv')\n",
    "rec9 = pd.read_csv('rec9.csv')\n",
    "print(rec1.shape, rec2.shape, rec3.shape, rec4.shape, rec5.shape, rec8.shape, rec9.shape)\n",
    "\n",
    "# Concatenate the 4 participants into one dataframe\n",
    "df = pd.concat([rec1, rec2, rec3, rec4, rec5, rec8, rec9], ignore_index=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Drop redundant indexing column\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Number of target statuses\n",
    "targets = df['Status'].unique()\n",
    "print('Number of Statuses: ' + str(len(targets)))\n",
    "\n",
    "# Separate dataframe into predictors and targets\n",
    "X = df[df.columns[:-1]]\n",
    "y = df[df.columns[-1]]\n",
    "\n",
    "# Split predictors and targets into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\justi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\sag.py:334: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  \"the coef_ did not converge\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression Classifier\n",
    "clf_log = LogisticRegression(random_state=0, solver='saga', multi_class='multinomial', max_iter=1000)\n",
    "\n",
    "# Parameters to tune\n",
    "params = {\n",
    "    'C': [10e-5, 10e-4, 10e-3, 10e-2, 10e-1]\n",
    "}\n",
    "\n",
    "# Grid Search for best parameter value\n",
    "grid_log = GridSearchCV(clf_log, params, cv=5, n_jobs=-1, return_train_score=True, iid=False)\n",
    "\n",
    "# Train Logistic Regression\n",
    "result_log = grid_log.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter value is: {'C': 0.0001}\n",
      "Training accuracy: 0.7161904761904762\n",
      "Testing accuracy: 0.7351598173515982\n"
     ]
    }
   ],
   "source": [
    "# Obtain accuracy for training and testing\n",
    "train_acc_log = result_log.score(X_train, y_train)\n",
    "test_acc_log = result_log.score(X_test, y_test)\n",
    "\n",
    "print('Best parameter value is: ' + str(result_log.best_params_))\n",
    "print('Training accuracy: ' + str(train_acc_log))\n",
    "print('Testing accuracy: ' + str(test_acc_log))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA Classifier\n",
    "clf_lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "# 5-fold cross validation\n",
    "kf = KFold(n_splits = 5, shuffle=True, random_state=0)\n",
    "\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "# Obtain and iterate over train and test indices\n",
    "for train_index, test_index in kf.split(X):\n",
    "    \n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    # Train LDA classifier\n",
    "    clf_lda.fit(X_train, y_train)\n",
    "    \n",
    "    # Obtain training and testing scores\n",
    "    train_scores.append(clf_lda.score(X_train, y_train))\n",
    "    test_scores.append(clf_lda.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training score: 0.7697289087150473\n",
      "Mean testing score: 0.7336957530534209\n"
     ]
    }
   ],
   "source": [
    "# Obtain training and testing accuracy\n",
    "train_acc_lda = np.mean(train_scores)\n",
    "test_acc_lda = np.mean(test_scores)\n",
    "\n",
    "print('Mean training score: ' + str(train_acc_lda))\n",
    "print('Mean testing score: ' + str(test_acc_lda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# Hyperparameters for RF classifier\n",
    "params = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000],\n",
    "    'max_depth': [10, 20, 30, 40, 50, 60, 70, 80 ,90, 100],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5]\n",
    "}\n",
    "\n",
    "# Randomized search to find optimal hyperparameters\n",
    "rand_rf = RandomizedSearchCV(clf_rf, params, n_jobs=-1, iid=False, cv=5, return_train_score=True)\n",
    "\n",
    "# Fit classifier\n",
    "result_rf = rand_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter values are: {'n_estimators': 400, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_depth': 80}\n",
      "Train Score: 0.9996191926884996\n",
      "Test Score: 0.9679878048780488\n"
     ]
    }
   ],
   "source": [
    "# Obtained optimal hyperparameter values\n",
    "print('Best parameter values are: ' + str(result_rf.best_params_))\n",
    "\n",
    "# Calculate accuracy of optimized RF model\n",
    "train_acc_rf = result_rf.score(X_train, y_train)\n",
    "test_acc_rf = result_rf.score(X_test, y_test)\n",
    "\n",
    "print('Train Score: ' + str(train_acc_rf))\n",
    "print('Test Score: ' + str(test_acc_rf))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
